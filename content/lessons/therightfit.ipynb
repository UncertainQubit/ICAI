{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Right Fit\n",
    "Overfitting is where a model matches the training data almost perfectly, so when the model is being created, it looks like a near 100% accuracy rate; however, it does poorly in validation and other new data because it is perfectly fitted to the data that it was trained on. Underfitting is the opposite, where a when the model is being built, the accuracies are very high, similar to overfitting, but the model is missing crucial categories that define trends seen in the data and just has a very specific set of comparisons.\n",
    "\n",
    "Neither are desirable in a model as it greatly reduces the accuracy of the model when it is used on either training data or a larger set of data that was not used for the initial creation of the model. This eliminates the model's usefulness as it will make inaccurate predictions about what it is processing because it is missing a broader understanding of trends/patterns in the data.\n",
    "\n",
    "For example, a model built using a basic decision tree to determine house prices like the one pictured created below could be overfit if it had so many end nodes that they were each incredibly specific (i.e. 1111 Sherwood Street will sell for \\$111,000). When this model is used on data it has never seen before it performs poorly because it was so specified to the data it was trained on. This leads to accuracy suffering as a whole. Conversely, a decision tree with only two end nodes (say \\$100,000 and \\$200,000) would be inaccurate because it can't recognize broader trends in the data and is focused on only one or two features. Neither makes for a very good ML model. In this chapter you will learn about identifying overfitting/underfitting and how to correct it.\n",
    "\n",
    "![](https://raw.githubusercontent.com/UncertainQubit/firstrepo/master/R3ywQsR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of overfitting can be seen in the model below. When you test the model purely on the data it was trained with it performs extremely well (extremely close estimates to actual housing costs). However, when tested on separate validation data the model has an extremely large margin of error. Below we will explore ways to help identify and correct overfitting and underfitting in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mean Average Error (MAE): $73\n",
      "Validation Mean Average Error (MAE): $32,541\n"
     ]
    }
   ],
   "source": [
    "# Code you have previously used to load data\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "# Path of the file to read\n",
    "iowa_file_path = 'https://raw.githubusercontent.com/UncertainQubit/firstrepo/master/train.csv'\n",
    "\n",
    "home_data = pd.read_csv(iowa_file_path)\n",
    "# Create target object and call it y\n",
    "y = home_data.SalePrice\n",
    "# Create X\n",
    "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
    "X = home_data[features]\n",
    "\n",
    "# Split into validation and training data\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Specify Model\n",
    "iowa_model = DecisionTreeRegressor(max_leaf_nodes = 10000)\n",
    "# Fit Model\n",
    "iowa_model.fit(train_X, train_y)\n",
    "\n",
    "# Make training predictions and calculate mean absolute error\n",
    "train_predictions = iowa_model.predict(train_X)\n",
    "train_mae = mean_absolute_error(train_predictions, train_y)\n",
    "print(\"Training Mean Average Error (MAE): ${:,.0f}\".format(train_mae))\n",
    "\n",
    "# Make validation predipoctions and calculate mean absolute error\n",
    "val_predictions = iowa_model.predict(val_X)\n",
    "val_mae = mean_absolute_error(val_predictions, val_y)\n",
    "print(\"Validation Mean Average Error (MAE): ${:,.0f}\".format(val_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Analysis\n",
    "One way to identify overfitting/underfitting is if your model performs great with training data but poorly with new testing data which is why it is important to have both a large training dataset and a large testing dataset. One of the primary ways ML engineers work to address this problem is through something called hyperparameter analysis. Essentially this means trying a bunch of different values for various parameters within a model (such as the number of end nodes). This can either be done as a semi-automated process (like we will below) or as a fully automated process using services like AWS Sagemaker. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "We have created the function `get_mae` for you which will generate the MAE for different versions of our model. Code for this function can be viewed in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds_val = model.predict(val_X)\n",
    "    mae = mean_absolute_error(val_y, preds_val)\n",
    "    return(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Compare Different Tree Sizes\n",
    "This loop tries the following values for *max_leaf_nodes* from a set of possible values. By trying a large variety of values and comparing the MAE the loop is able to identify what parameter minimizes inaccuracy. Remember that we are measuring MAE against our *testing* data, this allows us to determine what values minimize underfitting/overfitting and maximize the accuracy of our model when it is actually deployed in a real-world application. We will then print the optimal tree size and store it in the variable *best_tree_size*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best max_leaf_nodes = 82\n"
     ]
    }
   ],
   "source": [
    "candidate_max_leaf_nodes = list(range(2, 1000))\n",
    "scores = {leaf_size: get_mae(leaf_size, train_X, val_X, train_y, val_y) for leaf_size in candidate_max_leaf_nodes}\n",
    "best_tree_size = min(scores, key=scores.get)\n",
    "print('Best max_leaf_nodes = ' + str(best_tree_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Fit Model Using All Data\n",
    "You know the best tree size. If you were going to deploy this model in practice, you would make it even more accurate by using all of the data and keeping that tree size.  That is, you don't need to hold out the validation data now that you've made all your modeling decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa4d555c9e1446cb10a33ceb42441e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>IntSlider</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "IntSlider(value=1, description='Test', max=1000, min=1, step=10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "max_leaf_nodes 1 must be either None or larger than 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-ce96ff27d577>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfinal_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_leaf_nodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwidget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mfinal_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mcmbd_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mcmbd_mae\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmbd_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1122\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1125\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmax_leaf_nodes\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m             raise ValueError((\"max_leaf_nodes {0} must be either None \"\n\u001b[1;32m--> 248\u001b[1;33m                               \"or larger than 1\").format(max_leaf_nodes))\n\u001b[0m\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: max_leaf_nodes 1 must be either None or larger than 1"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "widget = widgets.IntSlider(min=1, max=1000, step=10, description='Test', readout=True, continuous_update=True)\n",
    "display(widget)\n",
    "final_model = DecisionTreeRegressor(max_leaf_nodes = widget.value, random_state = 0)\n",
    "final_model.fit(X,y)\n",
    "cmbd_predictions = final_model.predict(X)\n",
    "cmbd_mae = mean_absolute_error(cmbd_predictions, y)\n",
    "print(\"Combined Mean Average Error (MAE): ${:,.0f}\".format(cmbd_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "While error certainly exists with just a few simple steps we have managed to reduce the error by almost \\$14,644 which is a significant margin. In addition, that was all accomplished by changing only one parameter of many. Hyperparameter analysis is an essential tool in any ML engineer's toolkit because it allows for the identification and minimization of underfitting and overfitting which can result in significant penalalties to accuracy when actually deployed in the real world.\n",
    "\n",
    "### Great Job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
