---
interact_link: content/lessons/therightfit.ipynb
kernel_name: python3
kernel_path: content/lessons
has_widgets: false
title: |-
  Lessons
pagenum: 0
prev_page:
  url: 
next_page:
  url: 
suffix: .ipynb
search: model data overfitting training function tree validation very set models different values step maxleafnodes using fit where perfectly being created accuracy however poorly new because trained underfitting missing trends seen used not below performs extremely great testing large dataset getmae value accurate size even youve right matches almost looks near rate does fitted opposite built accuracies high similar crucial categories define just specific comparisons neither desirable greatly reduces either larger initial creation eliminates usefulness inaccurate predictions processing broader understanding patterns raw githubusercontent com uncertainqubit firstrepo master rywqsr png example test purely well close estimates actual housing costs tested separate extremel

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">Lessons</div>
</div>
    <div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="The-Right-Fit">The Right Fit<a class="anchor-link" href="#The-Right-Fit"> </a></h1><p>Overfitting is where a model matches the training data almost perfectly, so when the model is being created, it looks like a near 100% accuracy rate; however, it does poorly in validation and other new data because it is perfectly fitted to the data that it was trained on. Underfitting is the opposite, where a when the model is being built, the accuracies are very high, similar to overfitting, but the model is missing crucial categories that define trends seen in the data and just has a very specific set of comparisons.</p>
<p>Neither are desirable in a model as it greatly reduces the accuracy of the model when it is used on either training data or a larger set of data that was not used for the initial creation of the model. This eliminates the models usefulness as it will make inaccurate predictions about what it is processing because it is missing a broader understanding of trends/patterns in the data.</p>
<p><img src="https://raw.githubusercontent.com/UncertainQubit/firstrepo/master/R3ywQsR.png" alt=""></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>An example of overfitting can be seen in the model below. When you test the model purely on the data it was trained with it performs extremely well (extremely close estimates to actual housing costs). However, when tested on separate validation data the model has an extremel</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Code you have previously used to load data</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>


<span class="c1"># Path of the file to read</span>
<span class="n">iowa_file_path</span> <span class="o">=</span> <span class="s1">&#39;https://raw.githubusercontent.com/UncertainQubit/firstrepo/master/train.csv&#39;</span>

<span class="n">home_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">iowa_file_path</span><span class="p">)</span>
<span class="c1"># Create target object and call it y</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">home_data</span><span class="o">.</span><span class="n">SalePrice</span>
<span class="c1"># Create X</span>
<span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;LotArea&#39;</span><span class="p">,</span> <span class="s1">&#39;YearBuilt&#39;</span><span class="p">,</span> <span class="s1">&#39;1stFlrSF&#39;</span><span class="p">,</span> <span class="s1">&#39;2ndFlrSF&#39;</span><span class="p">,</span> <span class="s1">&#39;FullBath&#39;</span><span class="p">,</span> <span class="s1">&#39;BedroomAbvGr&#39;</span><span class="p">,</span> <span class="s1">&#39;TotRmsAbvGrd&#39;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">home_data</span><span class="p">[</span><span class="n">features</span><span class="p">]</span>

<span class="c1"># Split into validation and training data</span>
<span class="n">train_X</span><span class="p">,</span> <span class="n">val_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">val_y</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Specify Model</span>
<span class="n">iowa_model</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">)</span>
<span class="c1"># Fit Model</span>
<span class="n">iowa_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>

<span class="c1"># Make training predictions and calculate mean absolute error</span>
<span class="n">train_predictions</span> <span class="o">=</span> <span class="n">iowa_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">train_X</span><span class="p">)</span>
<span class="n">train_mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">train_predictions</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training MAE: $</span><span class="si">{:,.0f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_mae</span><span class="p">))</span>

<span class="c1"># Make validation predictions and calculate mean absolute error</span>
<span class="n">val_predictions</span> <span class="o">=</span> <span class="n">iowa_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">val_X</span><span class="p">)</span>
<span class="n">val_mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">val_predictions</span><span class="p">,</span> <span class="n">val_y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Validation MAE: $</span><span class="si">{:,.0f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">val_mae</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Training MAE: $62
Validation MAE: $29,298
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One way to identify overfitting/underfitting is if your model performs great with training data but poorly with new testing data which is why it is important to have both a large training dataset and a large testing dataset. One of the primary ways ML engineers work to address this problem is through something called hyperparameter analysis. Essential this means trying a bunch of different values for various parameters within a function (such as the number of end nodes).</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Exercise">Exercise<a class="anchor-link" href="#Exercise"> </a></h1><p>We have created the function <code>get_mae</code> for you which will generate the MAE for different versions of our model. Code for this function can be viewed in the cell below.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_mae</span><span class="p">(</span><span class="n">max_leaf_nodes</span><span class="p">,</span> <span class="n">train_X</span><span class="p">,</span> <span class="n">val_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">val_y</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_leaf_nodes</span><span class="o">=</span><span class="n">max_leaf_nodes</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>
    <span class="n">preds_val</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">val_X</span><span class="p">)</span>
    <span class="n">mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">val_y</span><span class="p">,</span> <span class="n">preds_val</span><span class="p">)</span>
    <span class="k">return</span><span class="p">(</span><span class="n">mae</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Step-1:-Compare-Different-Tree-Sizes">Step 1: Compare Different Tree Sizes<a class="anchor-link" href="#Step-1:-Compare-Different-Tree-Sizes"> </a></h2><p>Write a loop that tries the following values for <em>max_leaf_nodes</em> from a set of possible values.</p>
<p>Call the <em>get_mae</em> function on each value of max_leaf_nodes. Store the output in some way that allows you to select the value of <code>max_leaf_nodes</code> that gives the most accurate model on your data.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">candidate_max_leaf_nodes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">250</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">2000</span><span class="p">]</span>
<span class="c1"># Write loop to find the ideal tree size from candidate_max_leaf_nodes</span>
<span class="n">_</span>
<span class="n">scores</span> <span class="o">=</span> <span class="p">{</span><span class="n">leaf_size</span><span class="p">:</span> <span class="n">get_mae</span><span class="p">(</span><span class="n">leaf_size</span><span class="p">,</span> <span class="n">train_X</span><span class="p">,</span> <span class="n">val_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">val_y</span><span class="p">)</span> <span class="k">for</span> <span class="n">leaf_size</span> <span class="ow">in</span> <span class="n">candidate_max_leaf_nodes</span><span class="p">}</span>
<span class="n">best_tree_size</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">scores</span><span class="o">.</span><span class="n">get</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">best_tree_size</span><span class="p">)</span>
<span class="c1"># Store the best value of max_leaf_nodes (it will be either 5, 25, 50, 100, 250 or 500)</span>
<span class="n">best_tree_size</span> <span class="o">=</span> <span class="mi">100</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-6-36ea35ccb72c&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span> <span class="ansi-red-fg"># Write loop to find the ideal tree size from candidate_max_leaf_nodes</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span> _
<span class="ansi-green-fg">----&gt; 4</span><span class="ansi-red-fg"> </span>scores <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">{</span>leaf_size<span class="ansi-blue-fg">:</span> get_mae<span class="ansi-blue-fg">(</span>leaf_size<span class="ansi-blue-fg">,</span> train_X<span class="ansi-blue-fg">,</span> val_X<span class="ansi-blue-fg">,</span> train_y<span class="ansi-blue-fg">,</span> val_y<span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">for</span> leaf_size <span class="ansi-green-fg">in</span> candidate_max_leaf_nodes<span class="ansi-blue-fg">}</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> best_tree_size <span class="ansi-blue-fg">=</span> min<span class="ansi-blue-fg">(</span>scores<span class="ansi-blue-fg">,</span> key<span class="ansi-blue-fg">=</span>scores<span class="ansi-blue-fg">.</span>get<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      6</span> print<span class="ansi-blue-fg">(</span>best_tree_size<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">&lt;ipython-input-6-36ea35ccb72c&gt;</span> in <span class="ansi-cyan-fg">&lt;dictcomp&gt;</span><span class="ansi-blue-fg">(.0)</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span> <span class="ansi-red-fg"># Write loop to find the ideal tree size from candidate_max_leaf_nodes</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span> _
<span class="ansi-green-fg">----&gt; 4</span><span class="ansi-red-fg"> </span>scores <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">{</span>leaf_size<span class="ansi-blue-fg">:</span> get_mae<span class="ansi-blue-fg">(</span>leaf_size<span class="ansi-blue-fg">,</span> train_X<span class="ansi-blue-fg">,</span> val_X<span class="ansi-blue-fg">,</span> train_y<span class="ansi-blue-fg">,</span> val_y<span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">for</span> leaf_size <span class="ansi-green-fg">in</span> candidate_max_leaf_nodes<span class="ansi-blue-fg">}</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> best_tree_size <span class="ansi-blue-fg">=</span> min<span class="ansi-blue-fg">(</span>scores<span class="ansi-blue-fg">,</span> key<span class="ansi-blue-fg">=</span>scores<span class="ansi-blue-fg">.</span>get<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      6</span> print<span class="ansi-blue-fg">(</span>best_tree_size<span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">NameError</span>: name &#39;get_mae&#39; is not defined</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Step-2:-Fit-Model-Using-All-Data">Step 2: Fit Model Using All Data<a class="anchor-link" href="#Step-2:-Fit-Model-Using-All-Data"> </a></h2><p>You know the best tree size. If you were going to deploy this model in practice, you would make it even more accurate by using all of the data and keeping that tree size.  That is, you don't need to hold out the validation data now that you've made all your modeling decisions.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Fill in argument to make optimal size and uncomment</span>
<span class="n">final_model</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="n">best_tree_size</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># fit the final model and uncomment the next two lines</span>
<span class="n">final_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>DecisionTreeRegressor(criterion=&#39;mse&#39;, max_depth=None, max_features=None,
           max_leaf_nodes=100, min_impurity_decrease=0.0,
           min_impurity_split=None, min_samples_leaf=1,
           min_samples_split=2, min_weight_fraction_leaf=0.0,
           presort=False, random_state=0, splitter=&#39;best&#39;)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You've tuned this model and improved your results. But we are still using Decision Tree models, which are not very sophisticated by modern machine learning standards. In the next step you will learn to use Random Forests to improve your models even more.</p>
<h1 id="Great-Job!">Great Job!<a class="anchor-link" href="#Great-Job!"> </a></h1>
</div>
</div>
</div>
</div>

 


    </main>
    